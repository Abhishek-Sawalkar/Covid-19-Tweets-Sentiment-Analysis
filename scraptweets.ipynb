{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "check\n",
      "check_1\n",
      "Hyderabad\n",
      "check_1\n",
      "Warangal\n",
      "check_1\n",
      "Nizamabad\n",
      "check_1\n",
      "Khammam\n",
      "check_1\n",
      "Karimnagar\n",
      "check_1\n",
      "Agartala\n",
      "check_1\n",
      "Kailashahar\n",
      "check_1\n",
      "Kamalpur\n",
      "check_1\n",
      "Khowai\n",
      "check_1\n",
      "Kumarghat\n",
      "check_1\n",
      "Dehradun\n",
      "check_1\n",
      "Nainital\n",
      "check_1\n",
      "Haridwar\n",
      "check_1\n",
      "Roorkee\n",
      "check_1\n",
      "Rudrapur\n",
      "check_1\n",
      "Ghaziabad\n",
      "check_1\n",
      "Lucknow\n",
      "check_1\n",
      "Kanpur\n",
      "check_1\n",
      "Agra\n",
      "check_1\n",
      "Meerut\n",
      "check_1\n",
      "Kolkata\n",
      "check_1\n",
      "Asansol\n",
      "check_1\n",
      "Siliguri\n",
      "check_1\n",
      "Durgapur\n",
      "check_1\n",
      "Bardhaman\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#GetOldTweets3\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "This is a twitter scraping program.\n",
    "\"\"\"\n",
    "import twitter\n",
    "got = twitter.Api(consumer_key=<consumer key>,\n",
    "                  consumer_secret=<consumer secret>,\n",
    "                  access_token_key=<access token>,\n",
    "                  access_token_secret=<access token secret>)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "text_query = ['Lockdown','coronavirus', 'covid19'] # THIS 'Lockdown', \n",
    "start_date = \"2020-06-01\"\n",
    "end_date = \"2020-06-22\"\n",
    "count = 4000\n",
    "states = [\"Odisha\", \"Punjab\", \"Rajasthan\", \"Sikkim\", \"Tamil_Nadu\", \"Telangana\", \"Tripura\", \"Uttarakhand\", \"Uttar Pradesh\", \"West_Bengal\"]\n",
    "\n",
    "cities =     [[\"Hyderabad\", \"Warangal\", \"Nizamabad\", \"Khammam\", \"Karimnagar\"],\n",
    "              [\"Agartala\", \"Kailashahar\", \"Kamalpur\", \"Khowai\", \"Kumarghat\"],\n",
    "              [\"Dehradun\", \"Nainital\", \"Haridwar\", \"Roorkee\", \"Rudrapur\"],\n",
    "              [\"Ghaziabad\",\"Lucknow\", \"Kanpur\", \"Agra\", \"Meerut\"],\n",
    "              [\"Kolkata\", \"Asansol\", \"Siliguri\", \"Durgapur\", \"Bardhaman\"]]\n",
    "print(\"check\")\n",
    "#cities =     [[\"Bhubaneswar\", \"Cuttack\", \"Rourkela\", \"Brahmapur\", \"Sambalpur\"],\n",
    "#              [\"Chandigarh\", \"Ludhiana\", \"Amritsar\", \"Jalandhar\", \"Patiala\"],\n",
    "#              [\"Jaipur\", \"Jodhpur\", \"Kota\", \"Bikaner\", \"Ajmer\"],\n",
    "#              [\"Gangtok\", \"Dzongu\", \"Geyzing\", \"Mangan\", \"Namchi\"],\n",
    "#              [\"Chennai\", \"Coimbatore\", \"Madurai\", \"Tiruchirappalli\", \"Tiruppur\"]]\n",
    " # THIS\n",
    "\n",
    "# Function that pulls tweets based on a general search query and turns to csv file\n",
    "\n",
    "# Parameters: (text query you want to search), (max number of most recent tweets to pull from)\n",
    "\n",
    "def text_query_to_csv(text_query, count, place, start_date, end_date):\n",
    "    print(\"check_1\")\n",
    "    # Creation of query object\n",
    "    # Creation of query object\n",
    "    leng = 0\n",
    "    i=0\n",
    "    for query in text_query:\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query).setLang('en').setMaxTweets(count).setNear(place).setSince(start_date).setUntil(end_date)\n",
    "        # THIS (setSince(\"2020-05-01\").setUntil(\"2020-05-31\")) \n",
    "            # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "            # Creating list of chosen tweet data\n",
    "        text_tweets = [[place, query, tweet.date, tweet.text,tweet.id,tweet.username,tweet.geo,tweet.retweets,tweet.favorites,tweet.hashtags] for tweet in tweets]\n",
    "\n",
    "            # Creation of dataframe from tweets\n",
    "        if i == 0:\n",
    "            tweets_df = pd.DataFrame(text_tweets, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "        else:\n",
    "            temp = pd.DataFrame(text_tweets, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "            tweets_df = tweets_df.append(temp)  \n",
    "        i+=1\n",
    "        #tweets_df = pd.DataFrame(tweets_df, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "        #tweets_df = tweets_df.iloc[np.random.permutation(len(tweets_df))]\n",
    "        tweets_df = tweets_df.sample(frac=1).reset_index(drop=True)\n",
    "        if len(tweets_df) > count:\n",
    "            tweets_df = tweets_df.head(count)\n",
    "    return tweets_df   \n",
    " \n",
    "    \n",
    "df=pd.DataFrame()\n",
    "temp=pd.DataFrame()\n",
    "i=0\n",
    "print(\"check\")\n",
    "for city in cities:   \n",
    "    for place in city: \n",
    "        temp=text_query_to_csv(text_query, count, place, start_date, end_date)\n",
    "        if i==0:\n",
    "            df = temp\n",
    "        else:\n",
    "            df = df.append(temp)\n",
    "        print(place)\n",
    "        i+=1\n",
    "\n",
    "df.to_csv('june_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16019, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhubaneswar\n",
      "Cuttack\n",
      "Rourkela\n",
      "Brahmapur\n",
      "Sambalpur\n",
      "Chandigarh\n",
      "Ludhiana\n",
      "Amritsar\n",
      "Jalandhar\n",
      "Patiala\n",
      "Jaipur\n",
      "Jodhpur\n",
      "Kota\n",
      "Bikaner\n",
      "Ajmer\n",
      "Gangtok\n",
      "Dzongu\n",
      "Geyzing\n",
      "Mangan\n",
      "Namchi\n",
      "Chennai\n",
      "Coimbatore\n",
      "Madurai\n",
      "Tiruchirappalli\n",
      "Tiruppur\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "This is a twitter scraping program.\n",
    "\"\"\"\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "]\n",
    "\n",
    "\n",
    "text_query = ['Lockdown','coronavirus', 'covid19'] # THIS 'Lockdown', \n",
    "start_date = \"2020-06-01\"\n",
    "end_date = \"2020-06-22\"\n",
    "count = 5000\n",
    "states = [\"Odisha\", \"Punjab\", \"Rajasthan\", \"Sikkim\", \"Tamil_Nadu\", \"Telangana\", \"Tripura\", \"Uttarakhand\", \"Uttar Pradesh\", \"West_Bengal\"]\n",
    "\n",
    "cities =     [[\"Bhubaneswar\", \"Cuttack\", \"Rourkela\", \"Brahmapur\", \"Sambalpur\"],\n",
    "             [\"Chandigarh\", \"Ludhiana\", \"Amritsar\", \"Jalandhar\", \"Patiala\"],\n",
    "             [\"Jaipur\", \"Jodhpur\", \"Kota\", \"Bikaner\", \"Ajmer\"],\n",
    "             [\"Gangtok\", \"Dzongu\", \"Geyzing\", \"Mangan\", \"Namchi\"],\n",
    "             [\"Chennai\", \"Coimbatore\", \"Madurai\", \"Tiruchirappalli\", \"Tiruppur\"]]\n",
    " # THIS\n",
    "\n",
    "# Function that pulls tweets based on a general search query and turns to csv file\n",
    "\n",
    "# Parameters: (text query you want to search), (max number of most recent tweets to pull from)\n",
    "\n",
    "def text_query_to_csv(text_query, count, place, start_date, end_date):\n",
    "    # Creation of query object\n",
    "    # Creation of query object\n",
    "    leng = 0\n",
    "    i=0\n",
    "    for query in text_query:\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query).setLang('en').setMaxTweets(count).setNear(place).setSince(start_date).setUntil(end_date)\n",
    "        # THIS (setSince(\"2020-05-01\").setUntil(\"2020-05-31\")) \n",
    "            # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "            # Creating list of chosen tweet data\n",
    "        text_tweets = [[place, query, tweet.date, tweet.text,tweet.id,tweet.username,tweet.geo,tweet.retweets,tweet.favorites,tweet.hashtags] for tweet in tweets]\n",
    "\n",
    "            # Creation of dataframe from tweets\n",
    "        if i == 0:\n",
    "            tweets_df = pd.DataFrame(text_tweets, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "        else:\n",
    "            temp = pd.DataFrame(text_tweets, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "            tweets_df = tweets_df.append(temp)  \n",
    "        i+=1\n",
    "        #tweets_df = pd.DataFrame(tweets_df, columns = ['Place', 'Query', 'Datetime', 'Text','TweetID','username','geo','retweets','favourites','hashtags'])\n",
    "        #tweets_df = tweets_df.iloc[np.random.permutation(len(tweets_df))]\n",
    "        tweets_df = tweets_df.sample(frac=1).reset_index(drop=True)\n",
    "        if len(tweets_df) > count:\n",
    "            tweets_df = tweets_df.head(count)\n",
    "    return tweets_df   \n",
    " \n",
    "    \n",
    "df=pd.DataFrame()\n",
    "temp=pd.DataFrame()\n",
    "i=0\n",
    "for city in cities:   \n",
    "    for place in city: \n",
    "        temp=text_query_to_csv(text_query, count, place, start_date, end_date)\n",
    "        if i==0:\n",
    "            df = temp\n",
    "        else:\n",
    "            df = df.append(temp)\n",
    "        print(place)\n",
    "        i+=1\n",
    "\n",
    "df.to_csv('june_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15370, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31389, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"june_data.csv\")\n",
    "data2 = pd.read_csv(\"june_data2.csv\")\n",
    "data = pd.DataFrame(data)\n",
    "data2 = pd.DataFrame(data2)\n",
    "data = data.append(data2)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"may.csv\")\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = \"Text\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"may_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24489, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25278, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
